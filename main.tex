\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{chngcntr}

\counterwithin*{equation}{section}
\counterwithin*{equation}{subsection}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}
\title{Solution Manual for Supplemental Psets of MIT OCW 18.02SC Fall 2010}
\author{Lu YuXun}
\date{February 2017}


\linespread{1.5}

\newtheorem{theorem}{Theorem}

\begin{document}

\maketitle
This solution manual is for the problem set of MIT Opencourse 18.02SC Fall 2010. Answers of some problems that can be verified or solved easily are not given. All the answers are given by Yuxun LU from NAIST. Some arithmetic errors may occur. Not all answers are given with the corresponding problems. For those answers that original problems are not provided, please refer to supplemental problem sets (the Exercise on the webpage) at \url{https://ocw.mit.edu/courses/mathematics/18-02-multivariable-calculus-fall-2007/readings/supp_notes/}.
\section{Vectors and Matrices}
% Problem set 1: Vectors and Matrices
\subsection{A. Vectors}
\begin{theorem}[The Property of the Centroid of Triangles]
The centroid of a triangle is the point of intersection of its medians (the lines joining each vertex with the midpoint of the opposite side). The centroid divides each of the medians in the ratio 2:1. 
\end{theorem}
For the theorem above, refer \url{https://en.wikipedia.org/wiki/Centroid} ``Of triangle" part for details.

\begin{theorem}[A trick for matrices multiplication]
Suppose $\mathbf{A} = \begin{pmatrix}
\mathbf{a}_1 & \mathbf{a}_2 & ... & \mathbf{a}_n
\end{pmatrix}, \mathbf{B} = \begin{pmatrix}
\mathbf{b}_1 \\
\mathbf{b}_2 \\
... \\
\mathbf{b}_n
\end{pmatrix}$ where $\mathbf{a}_i$'s are row vectors and $\mathbf{b}_j$'s are column vectors ($1 \leq i,j \leq n$). $\mathbf{AB} = \mathbf{a}_1 \cdot \mathbf{b}_1 + \mathbf{a}_2 \cdot \mathbf{b}_2 + ... + \mathbf{a}_n \cdot \mathbf{b}_n = \sum_{i=1}^n \mathbf{a}_i \cdot \mathbf{b}_i$.
\end{theorem}
For the trick mentioned above, refer Gilbert Strang's ``Inroduction to Linear Algbera" (2nd edition) somewhere for details.

% Section 1A: Vectors
%
% 1A-12*
%
\textbf{1A-12*} Label the four vertices of a parallelogram in counterclockwise order as OPQR. Prove that the line segment from O to the midpoint of PQ intersects the diagonal PR in a point X that is 1/3 of the way from P to R.
\begin{figure}[htp!]
    \centering
    \includegraphics[width=50mm,scale=0.5]{Figure/1A-12.png}
    \caption{1A-12 Figure}
    \label{1A-12 Figure}
\end{figure}
%
% 1A-12* Solution
%
\begin{proof}
Suppose $\overrightarrow{OP} = \mathbf{a}$ and $\overrightarrow{OR} = \mathbf{b}$.
\\ We have: $\overrightarrow{RP} = \mathbf{a} - \mathbf{b}$ (1), $\overrightarrow{OM} = \mathbf{a} + \frac{1}{2} \mathbf{b}$ (2), $\overrightarrow{OX} = \mathbf{b} + \overrightarrow{RX}$ (3).
\\Let $\overrightarrow{OX} = u(\mathbf{a} + \frac{1}{2}\mathbf{b})$, $\overrightarrow{RX} = (1 - v)(\mathbf{a} - \mathbf{b})$.
\\ From (3) we can derive:
\\ $\mathbf{b} + \overrightarrow{RX} = \overrightarrow{OX} = u\mathbf{a} + \frac{u}{2}\mathbf{b} = \mathbf{b} + (1-v)\mathbf{a} - (1-v)\mathbf{b}$
that is, 
\begin{equation}
    \begin{cases}
    u = 1 - v
    \\ v = \frac{u}{2}
    \end{cases}
\end{equation}
Solve equations in (1), we have $u = \frac{2}{3}, v = \frac{1}{3}$. So $\overrightarrow{OX} = \frac{2}{3} \overrightarrow{OM}$ and $\overrightarrow{RX} = \frac{2}{3} \overrightarrow{RP}$, i.e. the intersection point X of RP and OM is 1/3 of the way from P to R.
\end{proof}
%
% 1A-13*
%
\textbf{1A-13*} a) Take a triangle $PQR$ in the plane; prove that as vectors $PQ+QR+RP = 0$.
\\ b) Continuing part a), let \textbf{A} be a vector the same length as $PQ$, but perpendicular to it, and pointing outside the triangle. Using similar vectors \textbf{B} and \textbf{C} for the other two sides, prove that \textbf{A} + \textbf{B} + \textbf{C} = \textbf{0}. (This only takes one sentence, and no computation.)
%
% 1A-13* Solution
%
\begin{proof}
a) $\because \overrightarrow{PQ} + \overrightarrow{QR} = \overrightarrow{PR}, \overrightarrow{PR} = -\overrightarrow{RP}$
\\ $\therefore \overrightarrow{PQ} + \overrightarrow{QR} + \overrightarrow{RP} = \mathbf{0}$
\\ b) Because the triangle constructed by the segment lines of \textbf{A} and \textbf{B} and \textbf{C} is the triangle $\bigtriangleup PQR$ rotated 90 degrees, since we have proved that the three vectors consisting of the three sides of $\bigtriangleup PQR$ add to \textbf{0}, \textbf{A} + \textbf{B} + \textbf{C} = \textbf{0}.
\end{proof}
%
% 1A-14*
%
\textbf{1A-14*} Generalize parts a) and b) of the previous exercise to a closed polygon in the plane which doesn't cross itself (i.e., one whose interior is a single region); label its vertices $P_1, P_2,...,P_n$ as you walk around it.
%
% 1A-14* Solution
%
\begin{proof}
We are going to use strong induction. 
\\Suppose $P(n)$: for a closed polygon with $n$ sides, $\overrightarrow{P_1P_2} + \overrightarrow{P_2P_3} + \overrightarrow{P_3P_4} + ... + \overrightarrow{P_{n-1}P_n} + \overrightarrow{P_nP_1} = \mathbf{0}$, $n \in \mathbb{N}^+ \wedge n \geq 3$.
\\ \textit{Base Case} P(3) is true, since we have proved it in \textbf{1A-13*} (a) part.
\\ \textit{Inductive Step} Suppose $P(1), P(2), ..., P(n)$ is true. For $P(n+1)$, we could add up arbitrary two adjacent vectors to eliminate one side, i.e. let $\overrightarrow{P_iP_{i+2}} = \overrightarrow{P_iP_{i+1}} + \overrightarrow{P_{i+1}P_{i+2}}$ where $ 3 \leq i \leq n+1 \wedge i \in \mathbb{N}^+$. So $P(n+1)$ decays to $P(n)$ and by our inductive assumption, $P(n)$ is true. Thus, $P(n+1)$ is true, i.e. $P(n) \implies P(n+1)$. So $P(n)$ is true for all positive nature number $n \geq 3$
\end{proof}
%
% 1A-15*
%
\textbf{1A-15*} Let $P_1, ..., P_n$ be the vertices of a regular n-gon in the plane, and $O$ its center; show without computation or coordinates that $\overrightarrow{OP_1} + \overrightarrow{OP_2} + ... + \overrightarrow{OP_n} = \mathbf{0}$.
\\ a) if $n$ is even;  b) if $n$ is odd.
%
% 1A-15* Solution
%
\begin{proof}
a) Suppose $n$ is a even number, two vectors $\overrightarrow{OP_i}$ and $\overrightarrow{OP_{i+n/2}}$ neutralized ($i \leq n/2 \wedge i \in \mathbb{N}^+$) because they have the same length/magnitude (because O is the n-gon's center) and opposite direction, thus $\overrightarrow{OP_1} + \overrightarrow{OP_2} + ... + \overrightarrow{OP_n} = \mathbf{0}$ when $n$ is a even number and $n \geq 4$.
\\ b) Suppose $n$ is an odd number, the vector $\overrightarrow{OP_i} + \overrightarrow{OP_{i+1}}$ is with the same length of the opposite vector $\overrightarrow{OP_j}$ that the point $P_j$ is opposite to the side where $\overrightarrow{OP_i}$ and $\overrightarrow{OP_{i+1}}$ towards because $\bigtriangleup P_iP_{i+1}P_j$ is a triangle and $O$ is the centroid of $\bigtriangleup P_iP_{i+1}P_j$ too. By the property of the centroid we have  $\overrightarrow{OP_j} = - (\overrightarrow{OP_i} + \overrightarrow{OP_{i+1}})$. For any n-gon that $n$ is a odd number and $n \geq 3$, for any vertex in that n-gon, there are two other different vertices of the n-gon that such three vertices form a triangle with $O$ as its centroid. Thus, $\overrightarrow{OP_1} + \overrightarrow{OP_2} + ... + \overrightarrow{OP_n} = \mathbf{0}$ where $n$ is an odd number and $n \geq 3$.
\end{proof}
%
% Section 1.2 B. Dot Product
%
\subsection{B. Dot Product}
%
% 1B-15*
%
\textbf{1B-15*} The \textbf{Cauchy-Schwarz inequality}
\\ a) Prove from the geometric definition of the dot product the following inequality for vectors in the plane or space; under what circumstances does equality hold?
\begin{equation}
    |\mathbf{A} \cdot \mathbf{B} | \leq |\mathbf{A}||\mathbf{B}|
\end{equation}
b) If the vectors are plane vectors, write out what this inequality says in terms of \textbf{i j}-components.
\\c) Give a different argument for the inequality (*) as follows (this argument generalizes to $n$-dimensional space):
\\i) for all values of $t$, we have (\textbf{A} + $t$\textbf{B}) $\cdot$ (\textbf{A} + $t$\textbf{B}) $\geq 0$.
\\ii) use the algebraic laws of the dot product to write the expression in (i) as a quadratic polynomial in $t$;
\\iii) by (i) this polynomial has at most one zero; this implies by the quadratic formula that its coefficients must satisfy a certain inequality - what is it?
%
% 1B-15* Solution
%
\begin{proof}
a) The component of \textbf{A} on the direction of \textbf{B} is great than or equal to the product of \textbf{A} and \textbf{B}'s lengths. The equality holds \textit{iff} \textbf{A} and \textbf{B} are in the same line, i.e. the angle between vector \textbf{A} and \textbf{B} is 180-degree or 0-degree.
\\b) Suppose $\mathbf{A} = (x_1, y_1)$ and $\mathbf{B} = (x_2, y_2)$. The equality says
\begin{equation}
    |x_1x_2 + y_1y_2| \leq \sqrt{(x_1^2 + y_1^2)(x_2^2+y_2^2)}
\end{equation}
\\c-i) Another argument for this statement is that, in any n-dimensional vector space, the length of the vectors in that vector space is great than or equal to 0.
\\c-ii) Suppose $\mathbf{A} = (x_1, x_2, x_3, ..., x_n)$ and $\mathbf{B} = (y_1, y_2, y_3, ..., y_n)$. Let $f(t) = (\mathbf{A} + t\mathbf{B})^2$, i.e. $f(t) = \sum_{i=1}^n (x_i + ty_i)^2 = \sum_{i=1}^n ( x_i^2 + t^2y_i^2 + 2tx_iy_i)$. By (i), $f(t) \geq 0$.
\\c-iii) $f(t) = t^2 \sum_{i=1}^ny_i^2 + 2t \sum_{i=1}^n x_iy_i + \sum_{i=1}^n x_i^2$. Re-write $f(t) = at^2 + bt + c$ where $c = \sum_{i=1}^n x_i^2$, $b = \sum_{i=1}^n 2x_iy_i$ and $a = \sum_{i=1}^n y_i^2$. By (i) we know $f(t) \geq 0$, then the coefficients $a,b,c$ must satisfy $b^2 - 4ac \leq 0$, i.e. $\sum_{i=1}^n (2x_iy_i)^2 - 4\sum_{i=1}^n x_i^2y_i^2 \leq 0 $. By arithmetic we know $b^2 - 4ac$ always equal to $0$, i.e. the coefficient satisfies the inequality.
\end{proof}
%
% 1C Determinants
%
\subsection{C. Determinants}
%
% 1C-8
%
\textbf{1C-8*} The base of a parallelepiped is a parallelogram whose edges are the vectors \textbf{b} and \textbf{c}, while its third edge is the vector \textbf{a}. (All three vectors have their tail at the same vertex; one calls them ``coterminal".)
\\ a) Show that the volume of the parallelepiped \textbf{abc} is $\pm\mathbf{a} \cdot ( \mathbf{b} \times \mathbf{c})$.
\\ b) Show that $\mathbf{a} \cdot (\mathbf{b} \times \mathbf{c})$ = the determinant whose rows are respectively the components of the vectors \textbf{a,b,c}.
\\ (These two parts prove the volume interpretation of a $3 \times 3$ determinant.)
% 
% 1C-8 Solution
%
\begin{proof}
a) $\because \mathbf{a} \cdot (\mathbf{b} \times \mathbf{c}) = |\mathbf{a}||\mathbf{b} \times \mathbf{c}| \cdot cos<\mathbf{a}, \mathbf{b} \times \mathbf{c}>$ 
\\ = $|\mathbf{a}|cos<\mathbf{a}, \mathbf{b} \times \mathbf{c}>||\mathbf{b}||\mathbf{c}|sin<\mathbf{b},\mathbf{c}>|$ 
\\ ($\because \mathbf{b} \times \mathbf{c}= |\mathbf{b}||\mathbf{c}|sin<\mathbf{b},\mathbf{c}>)$.
\\$\because \mathbf{b} \times \mathbf{c} \perp \mathbf{b}$ and $\mathbf{b} \times \mathbf{c} \perp \mathbf{c}$. 
\\ $\therefore \mathbf{a} \cdot (\mathbf{b} \times \mathbf{c})$ = component on the side of $(\mathbf{b} \times \mathbf{c})$ multiplies $|\mathbf{b} \times \mathbf{c}|$.
\\ $\because |\mathbf{b} \times \mathbf{c}|$ is the area of parallelogram where two sides are \textbf{b} and \textbf{c}.
\\ $\therefore |\mathbf{a}\cdot|\mathbf{b}\times\mathbf{c}||$ is the volumn of the parallelepiped, i.e. $\pm \mathbf{a}\cdot(\mathbf{b} \times \mathbf{c})$ is the volumn of the parallelepiped \textbf{abc}. $\square$
\\b) Because
\begin{align*} 
  \begin{vmatrix}
  a_1 & a_2 & a_3 \\
  b_1 & b_2 & b_3 \\
  c_1 & c_2 & c_3 \\
  \end{vmatrix}
= a_1(b_2 c_3 - b_3 c_2) - a_2(b_1 b_3 - b_3 c_1) + a_3(b_1 c_2 - b_2 c_1)
\\ = a_1(b_2 c_3 - b_3 c_2) + a_2(b_3 c_1 - b_1 b_3) + a_3(b_1 c_2 - b_2 c_1)
\end{align*}
and
\begin{align*}
    \begin{vmatrix}
    \mathbf{i} & \mathbf{j} & \mathbf{k} \\
    b_1 & b_2 & b_3 \\
    c_1 & c_2 & c_3 \\
    \end{vmatrix}
    = (b_2 b_3 - b_3 c_2)\mathbf{i} - (b_1 c_3 - b_3 c_1) \mathbf{j} + (b_1 c_2 - b_2 c_1) \mathbf{k}
\\  = (b_2 b_3 - b_3 c_2)\mathbf{i} + (b_3 c_1 - b_1 c_3) \mathbf{j} + (b_1 c_2 - b_2 c_1) \mathbf{k}
\end{align*}
Therefore,
\begin{align*}
    \mathbf{a} \cdot
    \begin{vmatrix}
      \mathbf{i} & \mathbf{j} & \mathbf{k} \\
      b_1 & b_2 & b_3 \\
      c_1 & c_2 & c_3 \\
    \end{vmatrix}
    =
    \begin{vmatrix}
    a_1 & a_2 & a_3 \\
    b_1 & b_2 & b_3 \\
    c_1 & c_2 & c_3 \\
    \end{vmatrix}
\end{align*}
\end{proof}
%
% 1E. Equations of Lines and Planes
%
%
% 1F. Matrix Algebra
%
\subsection{F. Matrix Algebra}
%
% 1F-10
%
\textbf{1F-10*} Suppose $A$ is a $2 \times 2$ orthogonal matrix, whose first entry is $a_{11} = cos \theta$. Fill in the rest of $A$. (There are four possibilities. Use Exercise 9.)
%
% 1F-10 Solution
%
\begin{proof}
There are more than 4 possibilities. The proof is as follows.
\\ Suppose
$A = 
\begin{pmatrix}
cos\theta & a_{12} \\
a_{21} & a_{22} \\
\end{pmatrix}$. \\
$AA^T = 
\begin{pmatrix}
cos\theta & a_{12} \\
a_{21} & a_{22} \\
\end{pmatrix}
\begin{pmatrix}
cos\theta & a_{21} \\
a_{12} & a_{22} \\
\end{pmatrix}
=
\begin{pmatrix}
cos^2\theta + a_{12}^2 & cos\theta a_{21} + a_{12}a_{22} \\
cos\theta a_{21} + a_{12}a_{22} & a_{21}^2 + a_{22}^2 \\
\end{pmatrix}
\\ \because AA^T = I, 
\\ \therefore$
\begin{equation}
 \begin{cases}
cos\theta a_{21} + a_{12} a_{22} = 0 \\
cos\theta a_{21} + a_{12} a_{22} = 0 \\
a_{21}^2 + a_{22}^2 = 1 \\
cos^2\theta + a_{12}^2 = 1 \\
\end{cases}
\implies
a_{12}^2 = sin^2\theta, a_{12} = \pm sin\theta
\end{equation}
- Case 1 $a_{12} = sin\theta$
\\ 
$
\begin{cases}
a_{21} = -sin\theta \\
a_{22} = cos\theta \\
\end{cases}
$
$
\begin{cases}
a_{21} = sin\theta \\
a_{22} = -cos\theta \\
\end{cases}
$
$
\begin{cases}
a_{21} = -sin\theta \\
a_{22} = cos(-\theta) \\
\end{cases}
$
$
\begin{cases}
a_{21} = sin(-\theta) \\
a_{22} = cos\theta \\
\end{cases}
$
\\Generally, we shall ensure $cos\theta a_{21} + sin\theta a_{22} = 0$ here (the $\sin \theta$ in the equation is $a_{12}$ because we are discussing the case that $a_{12} = sin\theta$.), by using trigonometric formula $cos\alpha sin\beta + cos\beta sin\alpha = sin(\alpha + \beta)$ here, we can derive that all
$
\begin{cases}
a_{21} = sin(2k\pi - \theta) \\
a_{22} = cos(2k\pi - \theta) \\
\end{cases}
(k \in \mathbb{N}^+)
$
satisfy the requirement that $AA^T = I$.
\\
\\- Case 2 $a_{12} = -sin\theta$
\\ Analogously, 
\\
$
\begin{cases}
a_{21} = sin\theta \\
a_{22} = cos\theta \\
\end{cases}
$
,
$
\begin{cases}
a_{21} = -sin\theta \\
a_{22} = -cos\theta \\
\end{cases}
$
and
$
\begin{cases}
a_{21} = cos(\frac{(2k+1) \pi}{2} - \theta)\\
a_{22} = sin(\frac{(2k+1) \pi}{2} - \theta)\\
\end{cases}
k \in \mathbb{N}
$
satisfy the requirement $AA^T = I$. (The third one uses $cos(\alpha + \beta) = cos\alpha cos\beta - sin\alpha sin\beta$.) 
\end{proof}
%
% 1F-11
%
\textbf{1F-11*} Show that if $A + B$ and $AB$ are defined, then
\\ a) $(A + B)^T = A^T + B^T$ (b) $(AB)^T = B^TA^T$
%
% 1F-11 Solution
%
\begin{proof}
$
(AB)^T = 
\begin{pmatrix}
a_1 b_1 & a_1 b_2 & ... & a_1 b_n \\
a_2 b_1 & a_2 b_2 & ... & a_2 b_n \\
... & ... & ... & ... \\
a_m b_1 & a_m b_2 & ... & a_m b_n \\
\end{pmatrix}^T
=
\begin{pmatrix}
a_1 b_1 & a_2 b_1 & ... & a_m b_1 \\
a_1 b_2 & a_2 b_2 & ... & a_m b_2 \\
... & ... & ... & ... \\
a_1 b_n & a_2 b_n & ... & a_m b_n \\
\end{pmatrix}
$
\\ where $a_i$ is a row vector of $A$ and $b_j$ is a column vector of $B$, $ 1 \leq i \leq m$ and $1 \leq j \leq n$.
\\
$
B^TA^T = 
\begin{pmatrix}
b_1^T a_1^T & b_1^T a_2^T & ... & b_1^T a_m^T \\
b_2^T a_1^T & b_2^T a_2^T & ... & b_2^T a_m^T \\
... & ... & ... & ... \\
b_n^T a_1^T & b_n^T a_2^T & ... & b_n^T a_m^T \\
\end{pmatrix}
\because a_i b_j = a_i^T b_j^T \therefore (AB)^T = B^TA^T.
$
\end{proof}
%
% 1G. Solving Square Systems; Inverse Matrices
%
\subsection{G. Solving Square Systems; Inverse Matrices}
For each of the following (1G-1* - 1G-2*), solve the equation $\mathbf{Ax} = \mathbf{b}$ by finding $\mathbf{A}^{-1}$.
%
% 1G-1
%
\\\textbf{1G-1*}
$
\mathbf{A} =
\begin{pmatrix}
3 & 1 & -1 \\
-1 & 2 & 0 \\
-1 & -1 & -1 \\
\end{pmatrix}
,
\mathbf{b} =
\begin{pmatrix}
8 \\ 3 \\ 0
\end{pmatrix}
$
%
% 1G-1 Solution
%
\begin{proof}
$
A^{-1} = -\frac{1}{10}
\begin{pmatrix}
-2 & 2 & 2 \\
-1 & -4 & 1 \\
3 & 2 & 7 \\
\end{pmatrix}
,
x = \begin{pmatrix}
1 \\ 2 \\ -3
\end{pmatrix}
$
\end{proof}
%
% 1G-2 
%
\textbf{1G-2*} a) 
$
A = \begin{pmatrix}
4 & 3 \\
3 & 2 \\
\end{pmatrix}
,
b = \begin{pmatrix}
-1 \\ 1
\end{pmatrix}
$
b)
$
A = 
\begin{pmatrix}
4 & 3 \\
3 & 2 \\
\end{pmatrix}
,
b = \begin{pmatrix}
2 & 3
\end{pmatrix}
$
%
% 1G-2 Solution
%
\begin{proof}
a) $A^{-1} = \begin{pmatrix}
-2 & 3 \\
3 & -4 \\
\end{pmatrix}
,
x = \begin{pmatrix}
5 \\ -7
\end{pmatrix}
$
b) $A^{-1} = \begin{pmatrix}
-2 & 3 \\
3 & -4 \\
\end{pmatrix}
,
x = \begin{pmatrix}
5 \\ -6
\end{pmatrix}
$
\end{proof}
%
% 1G-6
%
\textbf{1G-6* Another calculation of the inverse matrix.}
\\ If we know $A^{-1}$, we can solve the system $\mathbf{Ax} = \mathbf{y}$ for $\mathbf{x}$ by writing $\mathbf{x} = \mathbf{A}^{-1}\mathbf{y}$. But conversely, if we can solve by some other method (elimination, say) for $\mathbf{x}$ in terms of $\mathbf{y}$, getting $\mathbf{x} = \mathbf{B}y$, then the matrix $\mathbf{B} = \mathbf{A}^{-1}$, and we will have found $\mathbf{A}^{-1}$.
\\ This is a good method if $\mathbf{A}$ is an upper or lower triangular matrix - one with only zeros respectively below or above the main diagonal. To illustrate:
\\a) Let $A = \begin{pmatrix}
-1 & 1 & 3 \\
0 & 2 & -1 \\
0 & 0 & 1
\end{pmatrix}$; find $\mathbf{A}^{-1}$ by solving $\begin{cases}
-x_1 + x_2 + 3x_3 = y_1 \\
2x_2 - x_3 = y_2 \\
x_3 = y_3
\end{cases}$ for the $x_i$ in terms of the $y_i$ (start from the bottom and proceed upwards).
\\b) Calculate $\mathbf{A}^{-1}$ by the method given in the notes.
%
% 1G-6 Solution
%
\begin{proof}
$
\begin{cases}
x_1 = -y_1 + \frac{1}{2} y_2 + \frac{7}{2} y_3 \\
x_2 = \frac{1}{2}y_2 + \frac{1}{2} y_3 \\
x_3 = y_3\\
\end{cases}
\mathbf{B} = \mathbf{A}^{-1} =
\begin{pmatrix}
-1 & \frac{1}{2} & \frac{7}{2} \\
0 & \frac{1}{2} & \frac{1}{2} \\
0 & 0 & 1 \\
\end{pmatrix}
$
\end{proof}
%
% 1G-7
%
\textbf{1G-7*} Consider the rotation matrix $\mathbf{A}_{\theta} = \begin{pmatrix}
cos\theta & -sin\theta \\
sin\theta & cos\theta \\
\end{pmatrix}
$ corresponding to rotation of the $x$ and $y$ axes through the angle $\theta$. Calculate $\mathbf{A_{\theta}^{-1}}$ by the adjoint matrix method, and expalin why your answer looks the way it does.
%
% 1G-7 Solution
%
\begin{proof}
$\mathbf{A_{\theta}^{-1}} = \begin{pmatrix}
cos\theta & sin\theta \\
-sin\theta & cos\theta \\
\end{pmatrix}
$. Becasue $\mathbf{A_{\theta}}$ rotates a vector by $\theta$ degree. $\mathbf{A}_{\theta}^{-1} = \begin{pmatrix}
cos\theta & sin\theta \\
-sin\theta & cos\theta \\
\end{pmatrix}
=
\begin{pmatrix}
cos(-\theta) & -sin(-\theta) \\
sin(-\theta) & cos(-\theta) \\
\end{pmatrix}
$, so the $\mathbf{A}_{\theta}^{-1}$ rotates the vector rotated by $\mathbf{A}_{\theta}^{-1}$ $-\theta$ degrees, i.e. $\mathbf{A}_{\theta}^{-1}$ rotates the vector back to its origin position.
\end{proof}
%
% 1G-8
%
\textbf{1G-8*} a) Show: $\mathbf{A}$ is an orthogonal matrix (cf. Exercise 1F-9) if and only if $\mathbf{A}^{-1} = \mathbf{A}^T$.
\\ b) Illustrate with the matrix of exercise 7 above.
\\ c) Use (a) to show that if $\mathbf{A}$ and $\mathbf{B}$ are $n \times n$ orthogonal matrices, so is $\mathbf{AB}$.
%
% 1G-8 Solution
%
\begin{proof}
a) $\because \mathbf{A}\mathbf{A}^T = \mathbf{I} = \mathbf{A}\mathbf{A}^T
\\ \therefore \mathbf{A}\mathbf{A}^T = \mathbf{A}\mathbf{A}^{-1}, \mathbf{A}^{-1}(\mathbf{A}\mathbf{A}^{T}) = \mathbf{A}^{-1}(\mathbf{A}\mathbf{A}^{-1}) \implies \mathbf{A}^{T}\mathbf{I} = \mathbf{A}^{-1}\mathbf{I}, \mathbf{A}^{T} = \mathbf{A}^{-1}$.
\\b) $\because \mathbf{A}_{\theta}^T \mathbf{A} = \mathbf{I} \therefore \mathbf{A}_{\theta}$ is an orthogonal matrix.
\\c) $\because (\mathbf{AB})(\mathbf{AB})^{-1} = \mathbf{I}, \therefore \mathbf{A}^T\mathbf{A}\mathbf{B}(\mathbf{AB})^{-1} = \mathbf{A}^{T}$. By the question, we know $\mathbf{A}$ and $\mathbf{B}$ are orthogonal matrix. $\therefore \mathbf{B}(\mathbf{AB})^{-1} = \mathbf{A}^{T}$, $\mathbf{B}^{T}\mathbf{B}(\mathbf{AB})^{-1} = \mathbf{B}^T\mathbf{A}^{T}$, $(\mathbf{AB})^{-1} = \mathbf{B}^{T}\mathbf{A}^{T} = (\mathbf{AB})^T$. Therefore, if $\mathbf{A}$ and $\mathbf{B}$ are $n \times n$ orthogonal matrices, so is $\mathbf{AB}$.
\end{proof}
\textbf{1G-9*} a) Let $\mathbf{A}$ be a $3 \times 3$ matrix such that $|\mathbf{A}| \neq 0$. The notes construct a right-inverse $\mathbf{A}^{-1}$, that is, a matrix such that $\mathbf{A} \cdot \mathbf{A}^{-1} = \mathbf{I}$. SHow that every such matrix $\mathbf{A}$ also has a left inverse $\mathbf{B}$ (i.e., a matrix such that $\mathbf{BA} = \mathbf{I}$.
\\ b) Deduce that $\mathbf{B} = \mathbf{A}^{-1}$ by a one-line argument.
\begin{proof}

\end{proof}
\end{document}